{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d94da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " BACTERIA CLASSIFICATION - EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      " Loading data...\n",
      "Training set: (11608, 61)\n",
      "Validation set: (2901, 61)\n",
      "Classes: ['Basophil' 'Eosinophil' 'Lymphocyte' 'Monocyte' 'Neutrophil']\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 1: Comparing Different Models (with SMOTE)\n",
      "======================================================================\n",
      "\n",
      "Testing Logistic Regression...\n",
      "  F1-macro: 0.7558 (¬±0.0092)\n",
      "  Time: 23.0s\n",
      "\n",
      "Testing Naive Bayes...\n",
      "  F1-macro: 0.3822 (¬±0.0107)\n",
      "  Time: 0.6s\n",
      "\n",
      "Testing KNN...\n",
      "  F1-macro: 0.6407 (¬±0.0066)\n",
      "  Time: 1.7s\n",
      "\n",
      "Testing SVM...\n",
      "  F1-macro: 0.8381 (¬±0.0061)\n",
      "  Time: 653.7s\n",
      "\n",
      "Testing Random Forest...\n",
      "  F1-macro: 0.8099 (¬±0.0087)\n",
      "  Time: 49.1s\n",
      "\n",
      "Testing Extra Trees...\n",
      "  F1-macro: 0.7890 (¬±0.0055)\n",
      "  Time: 11.0s\n",
      "\n",
      "Testing Gradient Boosting...\n",
      "  F1-macro: 0.8108 (¬±0.0116)\n",
      "  Time: 40712.1s\n",
      "\n",
      "Testing XGBoost...\n",
      "  F1-macro: 0.8516 (¬±0.0073)\n",
      "  Time: 42.6s\n",
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY:\n",
      "======================================================================\n",
      "              Model  F1-Score      Std     Time (s)\n",
      "            XGBoost  0.851553 0.007252    42.631077\n",
      "                SVM  0.838123 0.006122   653.721462\n",
      "  Gradient Boosting  0.810799 0.011571 40712.083271\n",
      "      Random Forest  0.809855 0.008659    49.061376\n",
      "        Extra Trees  0.789048 0.005519    10.960807\n",
      "Logistic Regression  0.755774 0.009227    22.962722\n",
      "                KNN  0.640652 0.006566     1.660744\n",
      "        Naive Bayes  0.382176 0.010745     0.570865\n",
      "======================================================================\n",
      "Saved: experiment1_comparison.png\n",
      "\n",
      "üèÜ Best Model: XGBoost with F1=0.8516\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: Feature Selection Impact\n",
      "======================================================================\n",
      "\n",
      "Testing with 10 features...\n",
      "  F1-macro: 0.7549\n",
      "\n",
      "Testing with 20 features...\n",
      "  F1-macro: 0.7993\n",
      "\n",
      "Testing with 30 features...\n",
      "  F1-macro: 0.8224\n",
      "\n",
      "Testing with 40 features...\n",
      "  F1-macro: 0.8306\n",
      "\n",
      "Testing with 50 features...\n",
      "  F1-macro: 0.8566\n",
      "\n",
      "Testing with 61 features...\n",
      "  F1-macro: 0.8590\n",
      "\n",
      "======================================================================\n",
      " N_Features  F1-Score\n",
      "         10  0.754941\n",
      "         20  0.799335\n",
      "         30  0.822425\n",
      "         40  0.830582\n",
      "         50  0.856574\n",
      "         61  0.859031\n",
      "======================================================================\n",
      "Saved: experiment2_feature_selection.png\n",
      "\n",
      " Conclusion: Using 61 features gives best F1-Score of 0.8590\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                             ExtraTreesClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "FEAT_DIR = r\"C:\\Users\\htagi\\Downloads\\Back\\features\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\htagi\\Downloads\\Back\\experiments_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" BACTERIA CLASSIFICATION - EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==========================================\n",
    "# LOAD DATA\n",
    "# ==========================================\n",
    "print(\"\\n Loading data...\")\n",
    "X_train = np.load(os.path.join(FEAT_DIR, \"features_train.npy\"))\n",
    "y_train_raw = np.load(os.path.join(FEAT_DIR, \"labels_train.npy\"))\n",
    "X_val = np.load(os.path.join(FEAT_DIR, \"features_val.npy\"))\n",
    "y_val_raw = np.load(os.path.join(FEAT_DIR, \"labels_val.npy\"))\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_raw)\n",
    "y_val = le.transform(y_val_raw)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "\n",
    "# CV setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# ==========================================\n",
    "# EXPERIMENT 1: MODEL COMPARISON\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXPERIMENT 1: Comparing Different Models (with SMOTE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Create pipeline\n",
    "    if model_name in ['Logistic Regression', 'SVM', 'KNN']:\n",
    "        pipeline = ImbPipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('smote', SMOTE(random_state=RANDOM_STATE, k_neighbors=3)),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=RANDOM_STATE, k_neighbors=3)),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "    \n",
    "    # Cross-validation\n",
    "    scores = []\n",
    "    for fold, (tr_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "        X_tr, X_v = X_train[tr_idx], X_train[val_idx]\n",
    "        y_tr, y_v = y_train[tr_idx], y_train[val_idx]\n",
    "        \n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "        y_pred = pipeline.predict(X_v)\n",
    "        f1 = f1_score(y_v, y_pred, average='macro')\n",
    "        scores.append(f1)\n",
    "    \n",
    "    mean_f1 = np.mean(scores)\n",
    "    std_f1 = np.std(scores)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'F1-Score': mean_f1,\n",
    "        'Std': std_f1,\n",
    "        'Time (s)': elapsed\n",
    "    })\n",
    "    \n",
    "    print(f\"  F1-macro: {mean_f1:.4f} (¬±{std_f1:.4f})\")\n",
    "    print(f\"  Time: {elapsed:.1f}s\")\n",
    "\n",
    "# Results DataFrame\n",
    "df_results = pd.DataFrame(results).sort_values('F1-Score', ascending=False)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv(os.path.join(OUTPUT_DIR, 'experiment1_model_comparison.csv'), index=False)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = ['green' if i == 0 else 'steelblue' for i in range(len(df_results))]\n",
    "axes[0].barh(df_results['Model'], df_results['F1-Score'], color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('F1-Score (macro)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "for i, (model, score) in enumerate(zip(df_results['Model'], df_results['F1-Score'])):\n",
    "    axes[0].text(score + 0.01, i, f'{score:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "axes[1].barh(df_results['Model'], df_results['Time (s)'], color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Model Training Time', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'experiment1_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: experiment1_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "best_model_name = df_results.iloc[0]['Model']\n",
    "best_f1 = df_results.iloc[0]['F1-Score']\n",
    "print(f\"\\n Best Model: {best_model_name} with F1={best_f1:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# EXPERIMENT 2: FEATURE SELECTION\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2: Feature Selection Impact\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_counts = [10, 20, 30, 40, 50, 61]\n",
    "selection_results = []\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    print(f\"\\nTesting with {n_features} features...\")\n",
    "    \n",
    "    if n_features < 61:\n",
    "        selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "        X_selected = selector.fit_transform(X_train, y_train)\n",
    "    else:\n",
    "        X_selected = X_train\n",
    "    \n",
    "    pipeline = ImbPipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE, k_neighbors=3)),\n",
    "        ('clf', XGBClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1, verbosity=0))\n",
    "    ])\n",
    "    \n",
    "    scores = []\n",
    "    for tr_idx, val_idx in cv.split(X_selected, y_train):\n",
    "        X_tr, X_v = X_selected[tr_idx], X_selected[val_idx]\n",
    "        y_tr, y_v = y_train[tr_idx], y_train[val_idx]\n",
    "        \n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "        y_pred = pipeline.predict(X_v)\n",
    "        scores.append(f1_score(y_v, y_pred, average='macro'))\n",
    "    \n",
    "    mean_f1 = np.mean(scores)\n",
    "    selection_results.append({'N_Features': n_features, 'F1-Score': mean_f1})\n",
    "    print(f\"  F1-macro: {mean_f1:.4f}\")\n",
    "\n",
    "df_selection = pd.DataFrame(selection_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(df_selection.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save results\n",
    "df_selection.to_csv(os.path.join(OUTPUT_DIR, 'experiment2_feature_selection.csv'), index=False)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_selection['N_Features'], df_selection['F1-Score'], \n",
    "         marker='o', linewidth=2, markersize=10, color='steelblue')\n",
    "plt.xlabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('F1-Score (macro)', fontsize=12, fontweight='bold')\n",
    "plt.title('Feature Selection Impact on Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(df_selection['N_Features'])\n",
    "\n",
    "best_idx = df_selection['F1-Score'].idxmax()\n",
    "best_n = df_selection.loc[best_idx, 'N_Features']\n",
    "best_score = df_selection.loc[best_idx, 'F1-Score']\n",
    "plt.scatter([best_n], [best_score], color='red', s=200, zorder=5, \n",
    "           marker='*', edgecolors='black', linewidth=2)\n",
    "plt.text(best_n, best_score + 0.01, f'Best: {best_n} features\\nF1={best_score:.4f}',\n",
    "        ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'experiment2_feature_selection.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: experiment2_feature_selection.png\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n Conclusion: Using {best_n} features gives best F1-Score of {best_score:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# EXPERIMENT 3: HYPERPARAMETER TUNING\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948045fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
